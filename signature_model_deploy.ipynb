{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import random \n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import keras\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name='signature_classify'\n",
    "file_location='sign_classify_f2/export/cl_model.hdf5'\n",
    "tf_serving_model_out_location='gs://signature_classify/export/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the keras exported model\n",
    "To download files from gcs, you can either use the python api or gsutil command. Here we will use gsutil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying gs://signature_classify/job_dir/cl_model.hdf5...\n",
      "- [1 files][ 19.9 MiB/ 19.9 MiB]                                                \n",
      "Operation completed over 1 objects/19.9 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://signature_classify/job_dir/cl_model.hdf5 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the keras hdf5 model to Tensorflow SavedModel format and upload it to GCS\n",
    "\n",
    "### Step 1 --> First define the model and load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag_shape = (100, 100, 3)\n",
    "num_filters = 32            # No. of conv filters\n",
    "max_pool_size = (2,2)       # shape of max_pool\n",
    "conv_kernel_size = (3, 3)    # conv kernel shape\n",
    "imag_shape = imag_shape\n",
    "num_classes = 2\n",
    "drop_prob = 0.5             # fraction to drop (0-1.0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=num_filters, kernel_size=(conv_kernel_size), input_shape=imag_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=max_pool_size))\n",
    "\n",
    "model.add(Conv2D(filters = num_filters*2, kernel_size=(conv_kernel_size), input_shape=imag_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=max_pool_size))\n",
    "\n",
    "model.add(Conv2D(filters = num_filters*4, kernel_size=(conv_kernel_size), input_shape=imag_shape, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=max_pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))  #Fully connected layer \n",
    "\n",
    "model.add(Dropout(drop_prob))\n",
    "\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.load_weights('cl_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 --> Export the model to SavedModel format using the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: signature_classify/export/version_1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import model_from_config, Sequential\n",
    "\n",
    "K.set_learning_phase(0) \n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "\n",
    "new_model = Sequential.from_config(config)\n",
    "new_model.set_weights(weights)\n",
    "\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "\n",
    "%rm -rf signature_classify/export/version_1/\n",
    "export_path = 'signature_classify/export/version_1/'\n",
    "\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "signature = predict_signature_def(inputs={'images': new_model.input},\n",
    "                                  outputs={'scores': new_model.output})\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={'serving_default': signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 --> Verify the exported SavedModel using saved_model_cli api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['images'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 100, 100, 3)\r\n",
      "        name: conv2d_input_1:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['scores'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 2)\r\n",
      "        name: dense_1_1/Sigmoid:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir signature_classify/export/version_1 --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 --> If you got an output like this, you can proceed to the next step. Now upload the folder where your SavedModel is written to gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://signature_classify/.DS_Store [Content-Type=application/octet-stream]...\n",
      "Copying file://signature_classify/export/.DS_Store [Content-Type=application/octet-stream]...\n",
      "Copying file://signature_classify/export/version_1/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://signature_classify/export/version_1/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "| [4 files][ 13.3 MiB/ 13.3 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m -o ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://signature_classify/export/version_1/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "/ [5 files][ 13.3 MiB/ 13.3 MiB]                                                \n",
      "Operation completed over 5 objects/13.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r signature_classify gs://signature_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the  model on Google Cloud ML-Engine\n",
    "### Step 1 --> Creating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_NAME=signature_classifier_model\n",
      "env: MODEL_PATH=gs://signature_classify/signature_classify/export/version_1\n",
      "\u001b[1;33mWARNING:\u001b[0m `--regions` flag will soon be required. Please explicitly specify a region. Using [us-central1] by default.\n",
      "Created ml engine model [projects/searce-sandbox/models/signature_classifier_model].\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_NAME signature_classifier_model\n",
    "%env MODEL_PATH gs://signature_classify/signature_classify/export/version_1\n",
    "\n",
    "#CREATE MODEL\n",
    "!gcloud ml-engine models create $MODEL_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 --> Creating version for the model you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine versions create \"version_3\" --model $MODEL_NAME --origin $MODEL_PATH \\\n",
    "    --python-version 3.5 --runtime-version 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 --> Serving the model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "#Resize the original image.\n",
    "img=Image.open('image.jpg')\n",
    "img = img.resize((100, 100), Image.ANTIALIAS)\n",
    "img.save('new_image.jpg')\n",
    "\n",
    "img=cv2.imread('new_image.jpg')\n",
    "input_dict={'images': img.tolist()}\n",
    "\n",
    "import json\n",
    "with open('test_data.json', 'w') as outfile:\n",
    "    json.dump(input_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES\r\n",
      "[1.0, 0.0]\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine predict --model $MODEL_NAME --version version_3 --json-instances test_data.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
